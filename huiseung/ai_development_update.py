# -*- coding: utf-8 -*-
"""AI_Development.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k7Q_IJf8HUhITi44NXfvWzWgxdTO9pBA
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import StratifiedKFold, cross_val_score, GroupKFold
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier  # 추가: CatBoost
import optuna  # 추가: 하이퍼파라미터 최적화
from scipy.stats import mode
from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_selection import VarianceThreshold, SelectFromModel, RFECV
from sklearn.utils.class_weight import compute_sample_weight
from torch.utils.data import WeightedRandomSampler
from sklearn.ensemble import StackingClassifier, VotingClassifier  # 추가: 앙상블 기법
from imblearn.over_sampling import SMOTE, ADASYN  # 추가: 불균형 데이터 처리

import os
os.environ["LOKY_MAX_CPU_COUNT"] = "4"
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# Custom Dataset for PyTorch with Weights
class FertilityDataset(Dataset):
    def __init__(self, features, targets=None, weights=None):
        if isinstance(features, (pd.DataFrame, pd.Series)):
            features = features.values
        self.features = torch.FloatTensor(features)

        if targets is not None:
            if isinstance(targets, pd.Series):
                targets = targets.values
            self.targets = torch.FloatTensor(targets)

            # 가중치 추가
            if weights is not None:
                if isinstance(weights, pd.Series):
                    weights = weights.values
                self.weights = torch.FloatTensor(weights)
            else:
                self.weights = None
        else:
            self.targets = None
            self.weights = None

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        if self.targets is not None:
            if self.weights is not None:
                return self.features[idx], self.targets[idx], self.weights[idx]
            return self.features[idx], self.targets[idx]
        return self.features[idx]

# 개선된 학습 함수
def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=50, scheduler=None):
    best_val_loss = float('inf')
    best_model = None

    for epoch in range(epochs):
        # Training Phase
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for batch in train_loader:
            if len(batch) == 3:  # 가중치가 포함된 경우
                features, targets, weights = batch
                features, targets, weights = features.to(device), targets.to(device), weights.to(device)
                weighted_criterion = lambda pred, y: (criterion(pred, y.view(-1, 1)) * weights.view(-1, 1)).mean()
            else:
                features, targets = batch
                features, targets = features.to(device), targets.to(device)
                weighted_criterion = criterion

            optimizer.zero_grad()
            outputs = model(features)
            loss = weighted_criterion(outputs, targets.view(-1, 1))
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = (outputs.data > 0.5).float()
            train_total += targets.size(0)
            train_correct += (predicted.view(-1) == targets).sum().item()

        avg_train_loss = train_loss / len(train_loader)
        train_accuracy = 100 * train_correct / train_total

        # Validation Phase
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        val_preds = []
        val_true = []

        with torch.no_grad():
            for features, targets in val_loader:
                features, targets = features.to(device), targets.to(device)
                outputs = model(features)
                loss = criterion(outputs, targets.view(-1, 1))

                val_loss += loss.item()
                predicted = (outputs.data > 0.5).float()
                val_preds.extend(outputs.cpu().numpy().ravel())
                val_true.extend(targets.cpu().numpy().ravel())
                val_total += targets.size(0)
                val_correct += (predicted.view(-1) == targets).sum().item()

        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = 100 * val_correct / val_total
        val_auc = roc_auc_score(val_true, val_preds)

        # 최적 모델 저장
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model = model.state_dict().copy()

        if scheduler is not None:
            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                scheduler.step(avg_val_loss)
            else:
                scheduler.step()

        print(f'Epoch [{epoch+1}/{epochs}]')
        print(f'Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')
        print(f'Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, Val AUC: {val_auc:.4f}')
        print('-' * 60)

    # 최종적으로 best model 사용
    model.load_state_dict(best_model)
    return model

# Neural Network Models
# 개선된 Transformer Model: Attention 메커니즘 향상, 정규화 추가
class EnhancedTransformerModel(nn.Module):
    def __init__(self, input_dim, num_heads=4, num_layers=3, dropout=0.3):
        super().__init__()
        self.input_norm = nn.LayerNorm(input_dim)  # 입력 정규화 추가

        # Multi-scale feature extraction
        self.embedding1 = nn.Linear(input_dim, 128)
        self.embedding2 = nn.Linear(input_dim, 64)
        self.embedding3 = nn.Linear(input_dim, 32)  # 새로운 스케일 추가

        encoder_layer1 = nn.TransformerEncoderLayer(
            d_model=128,
            nhead=num_heads,
            dropout=dropout,
            activation='gelu',
            batch_first=True
        )
        encoder_layer2 = nn.TransformerEncoderLayer(
            d_model=64,
            nhead=num_heads//2,
            dropout=dropout,
            activation='gelu',
            batch_first=True
        )
        encoder_layer3 = nn.TransformerEncoderLayer(
            d_model=32,
            nhead=2,
            dropout=dropout,
            activation='gelu',
            batch_first=True
        )

        self.transformer1 = nn.TransformerEncoder(encoder_layer1, num_layers)
        self.transformer2 = nn.TransformerEncoder(encoder_layer2, num_layers)
        self.transformer3 = nn.TransformerEncoder(encoder_layer3, num_layers)

        # 더 복잡한 Classifier 구조
        self.fc = nn.Sequential(
            nn.Linear(224, 128),  # 128+64+32=224
            nn.LayerNorm(128),
            nn.SELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.SELU(),
            nn.Dropout(dropout),
            nn.Linear(64, 32),
            nn.LayerNorm(32),
            nn.SELU(),
            nn.Dropout(dropout),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

        # 초기화 추가
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        x = self.input_norm(x)

        x1 = self.embedding1(x).unsqueeze(1)
        x2 = self.embedding2(x).unsqueeze(1)
        x3 = self.embedding3(x).unsqueeze(1)

        x1 = self.transformer1(x1).squeeze(1)
        x2 = self.transformer2(x2).squeeze(1)
        x3 = self.transformer3(x3).squeeze(1)

        x_combined = torch.cat([x1, x2, x3], dim=1)
        return self.fc(x_combined)

# 개선된 CNN Model: Residual 연결과 더 효과적인 특성 추출
class EnhancedCNNModel(nn.Module):
    def __init__(self, input_dim, dropout=0.3):
        super().__init__()

        def conv_block(in_channels, out_channels, kernel_size=3, dilation=1):
            return nn.Sequential(
                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2*dilation, dilation=dilation),
                nn.BatchNorm1d(out_channels),
                nn.SELU(),
                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2),
                nn.BatchNorm1d(out_channels),
                nn.SELU(),
            )

        # Multi-scale feature extraction with dilated convolutions
        self.input_proj = nn.Conv1d(1, 32, kernel_size=1)
        self.proj1 = nn.Conv1d(32, 64, kernel_size=1)
        self.proj2 = nn.Conv1d(64, 128, kernel_size=1)

        self.conv1 = conv_block(32, 64, dilation=1)
        self.conv2 = conv_block(64, 128, dilation=2)
        self.conv3 = conv_block(128, 256, dilation=4)

        # Attention mechanism
        self.attention = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(256, 64),
            nn.SELU(),
            nn.Linear(64, 256),
            nn.Sigmoid()
        )

        self.global_pool = nn.AdaptiveAvgPool1d(8)
        self.max_pool = nn.AdaptiveMaxPool1d(8)

        self.fc = nn.Sequential(
            nn.Linear(256 * 16, 512),  # 더 넓은 fully connected layer
            nn.LayerNorm(512),
            nn.SELU(),
            nn.Dropout(dropout),
            nn.Linear(512, 128),
            nn.LayerNorm(128),
            nn.SELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 32),
            nn.LayerNorm(32),
            nn.SELU(),
            nn.Dropout(dropout),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

        # Weight 초기화
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.input_proj(x)

        # 첫 번째 residual block
        identity = x
        x = self.conv1(x)
        identity = self.proj1(identity)
        x = x + identity

        # 두 번째 residual block
        identity = x
        x = self.conv2(x)
        identity = self.proj2(identity)
        x = x + identity

        # 세 번째 block + attention
        x = self.conv3(x)

        # 채널 어텐션 적용
        attn = self.attention(x)
        x = x * attn.unsqueeze(2)

        # 다양한 pooling 결과 결합
        avg_pooled = self.global_pool(x)
        max_pooled = self.max_pool(x)
        x = torch.cat([avg_pooled, max_pooled], dim=2)

        x = x.view(x.size(0), -1)
        return self.fc(x)

# 새로운 모델: GRU 기반 모델 추가
class GRUModel(nn.Module):
    def __init__(self, input_dim, hidden_dim=128, num_layers=2, dropout=0.3):
        super().__init__()
        self.input_norm = nn.LayerNorm(input_dim)

        self.gru = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0
        )

        self.attention = nn.Sequential(
            nn.Linear(hidden_dim*2, 1),
            nn.Tanh(),
            nn.Softmax(dim=1)
        )

        self.fc = nn.Sequential(
            nn.Linear(hidden_dim*2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.SELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.LayerNorm(hidden_dim//2),
            nn.SELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim//2, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.input_norm(x)
        x = x.unsqueeze(1)  # (batch, 1, features)

        output, _ = self.gru(x)  # (batch, seq_len, hidden*2)

        # Attention weights
        attention_weights = self.attention(output)  # (batch, seq_len, 1)
        context = torch.sum(attention_weights * output, dim=1)  # (batch, hidden*2)

        return self.fc(context)

class EnhancedFertilityEnsemble:
    def __init__(self, n_components=150, use_optuna=False):
        self.n_components = n_components
        self.scaler = RobustScaler()  # StandardScaler에서 RobustScaler로 변경
        self.pca = PCA(n_components=n_components, svd_solver='randomized')
        self.transformer = None
        self.cnn = None
        self.gru = None  # 새로운 GRU 모델 추가
        self.lgbm_classifier = None
        self.xgb_classifier = None
        self.catboost_classifier = None  # CatBoost 모델 추가
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.use_optuna = use_optuna
        self.best_weights = None  # 최적 가중치 저장

    def prepare_data(self, X, y=None, is_training=True):
    # 중복 열 확인 및 제거
    if isinstance(X, pd.DataFrame):
        # 중복 열 확인
        duplicated_cols = X.columns[X.columns.duplicated()].tolist()
        if duplicated_cols:
            print(f"중복된 열 발견: {duplicated_cols}")
            # 중복된 열 제거 (첫 번째 인스턴스만 유지)
            X = X.loc[:, ~X.columns.duplicated(keep='first')]
            print(f"중복 열을 제거했습니다. 현재 열 수: {X.shape[1]}")

        # DataFrame 최적화 (defragmentation)
        X = X.copy()

        if 'ID' in X.columns:
            X = X.drop('ID', axis=1)
        elif 'id' in X.columns:
            X = X.drop('id', axis=1)

        numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns

        if is_training:
            # Feature engineering: 다항식 특성
            self.poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)
            X_poly = self.poly.fit_transform(X[numeric_cols])
            self.poly_feature_names = self.poly.get_feature_names_out(numeric_cols)
            X_poly_df = pd.DataFrame(X_poly, columns=self.poly_feature_names)

            # 통계 특성 추가 (새로운 특성 생성)
            feature_dfs = [X]  # 원본 특성

            # 각 수치형 특성에 대한 파생 특성 생성
            squared_features = pd.DataFrame()
            cubed_features = pd.DataFrame()
            log_features = pd.DataFrame()

            for col in numeric_cols:
                if col in X.columns:  # 중복 체크
                    squared_features[f'{col}_squared'] = X[col] ** 2
                    cubed_features[f'{col}_cubed'] = X[col] ** 3
                    log_features[f'{col}_log'] = np.log1p(np.abs(X[col]))

            # 분산 임계값 기반 특성 선택
            self.selector = VarianceThreshold(threshold=0.01)
            X_poly_selected = self.selector.fit_transform(X_poly_df)
            self.selected_features = np.array(self.poly_feature_names)[self.selector.get_support()]
            X_poly_selected_df = pd.DataFrame(X_poly_selected, columns=self.selected_features)

            # 모든 특성을 한 번에 결합 (pd.concat 사용)
            feature_dfs.extend([squared_features, cubed_features, log_features, X_poly_selected_df])
            all_features = pd.concat(feature_dfs, axis=1)

            # 결합 후 중복 확인
            if all_features.columns.duplicated().any():
                print(f"특성 결합 후 중복 열 발견: {all_features.columns[all_features.columns.duplicated()].tolist()}")
                all_features = all_features.loc[:, ~all_features.columns.duplicated(keep='first')]
                print(f"최종 특성에서 중복 제거 완료. 현재 열 수: {all_features.shape[1]}")

            # 특성 중요도 기반 선택 (LGBM 사용)
            if y is not None:
                temp_lgbm = LGBMClassifier(n_estimators=100, random_state=42)
                self.feature_selector = SelectFromModel(temp_lgbm, threshold='median')
                self.feature_selector.fit(all_features, y)
                X_selected = self.feature_selector.transform(all_features)
                # 선택된 특성의 인덱스 저장
                self.selected_indices = self.feature_selector.get_support()
                # 선택된 열 이름 저장
                self.selected_columns = all_features.columns[self.selected_indices].tolist()
                print(f"특성 선택 후 남은 특성 수: {len(self.selected_columns)}")
            else:
                # y가 없을 경우 (테스트 데이터)
                X_selected = all_features.values
        else:
            # 예측 시 저장된 변환 사용
            feature_dfs = [X]  # 원본 특성

            # 동일한 파생 특성 생성
            squared_features = pd.DataFrame()
            cubed_features = pd.DataFrame()
            log_features = pd.DataFrame()

            for col in numeric_cols:
                if col in X.columns:  # 중복 체크
                    squared_features[f'{col}_squared'] = X[col] ** 2
                    cubed_features[f'{col}_cubed'] = X[col] ** 3
                    log_features[f'{col}_log'] = np.log1p(np.abs(X[col]))

            # 다항식 특성 생성
            X_poly = self.poly.transform(X[numeric_cols])
            X_poly_df = pd.DataFrame(X_poly, columns=self.poly_feature_names)
            X_poly_selected = self.selector.transform(X_poly_df)
            X_poly_selected_df = pd.DataFrame(X_poly_selected, columns=self.selected_features)

            # 모든 특성을 한 번에 결합
            feature_dfs.extend([squared_features, cubed_features, log_features, X_poly_selected_df])
            all_features = pd.concat(feature_dfs, axis=1)

            # 중복 열 제거
            if all_features.columns.duplicated().any():
                all_features = all_features.loc[:, ~all_features.columns.duplicated(keep='first')]

            # 학습 데이터셋에서 선택된 열만 사용
            if hasattr(self, 'selected_columns'):
                # 선택된 열 중 현재 데이터프레임에 존재하는 열만 사용
                available_cols = [col for col in self.selected_columns if col in all_features.columns]
                all_features = all_features[available_cols]
                X_selected = all_features.values
            else:
                # feature_selector 적용
                X_selected = self.feature_selector.transform(all_features)

    else:
        # X가 이미 numpy 배열인 경우
        X_selected = X

    # 데이터 스케일링 및 차원 축소
    if is_training:
        X_scaled = self.scaler.fit_transform(X_selected)
        # 충분한 분산을 설명하도록 PCA 성분 수 조정
        explained_var = 0.95
        self.pca = PCA(n_components=min(X_scaled.shape[1], self.n_components), svd_solver='randomized')
        X_pca = self.pca.fit_transform(X_scaled)

        # 95% 분산을 설명하는 성분 수 결정
        cumsum = np.cumsum(self.pca.explained_variance_ratio_)
        self.n_components_95 = np.argmax(cumsum >= explained_var) + 1
        print(f"95% 분산을 설명하는 PCA 성분 수: {self.n_components_95}")

        # 차원 축소된 데이터만 유지
        X_pca = X_pca[:, :self.n_components_95]

        # SMOTE로 불균형 데이터 처리 (훈련 데이터에만 적용)
        if y is not None and len(np.unique(y)) > 1:
            try:
                smote = SMOTE(random_state=42)
                X_pca, y = smote.fit_resample(X_pca, y)
                X_scaled, _ = smote.fit_resample(X_scaled, y)
                print("SMOTE 적용 완료: 클래스 분포 균형화")
            except Exception as e:
                print(f"SMOTE 적용 실패: {e}")
    else:
        X_scaled = self.scaler.transform(X_selected)
        X_pca = self.pca.transform(X_scaled)
        if hasattr(self, 'n_components_95'):
            X_pca = X_pca[:, :self.n_components_95]  # 훈련 시 결정된 성분 수만큼만 사용
        else:
            # 안전장치: n_components_95가 없는 경우
            X_pca = X_pca[:, :min(100, X_pca.shape[1])]  # 기본값으로 100개 성분 사용

    return X_pca, X_scaled

    def optimize_hyperparams(self, X_train, y_train, X_val, y_val):
        """Optuna를 사용한 하이퍼파라미터 최적화"""
        def objective(trial):
            # LightGBM 하이퍼파라미터
            lgbm_params = {
                'n_estimators': trial.suggest_int('lgbm_n_estimators', 500, 2000),
                'learning_rate': trial.suggest_float('lgbm_lr', 0.005, 0.05, log=True),
                'num_leaves': trial.suggest_int('lgbm_num_leaves', 20, 100),
                'max_depth': trial.suggest_int('lgbm_max_depth', 5, 15),
                'min_child_samples': trial.suggest_int('lgbm_min_child_samples', 10, 50),
                'subsample': trial.suggest_float('lgbm_subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('lgbm_colsample', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('lgbm_alpha', 0.01, 10.0, log=True),
                'reg_lambda': trial.suggest_float('lgbm_lambda', 0.01, 10.0, log=True),
                'random_state': 42
            }

            # XGBoost 하이퍼파라미터
            xgb_params = {
                'n_estimators': trial.suggest_int('xgb_n_estimators', 500, 2000),
                'learning_rate': trial.suggest_float('xgb_lr', 0.005, 0.05, log=True),
                'max_depth': trial.suggest_int('xgb_max_depth', 4, 12),
                'min_child_weight': trial.suggest_int('xgb_min_child_weight', 1, 10),
                'gamma': trial.suggest_float('xgb_gamma', 0.0, 1.0),
                'subsample': trial.suggest_float('xgb_subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('xgb_colsample', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('xgb_alpha', 0.01, 10.0, log=True),
                'reg_lambda': trial.suggest_float('xgb_lambda', 0.01, 10.0, log=True),
                'random_state': 42,
                'use_label_encoder': False,
                'eval_metric': 'logloss'
            }

            # CatBoost 하이퍼파라미터
            cat_params = {
                'iterations': trial.suggest_int('cat_iterations', 500, 2000),
                'learning_rate': trial.suggest_float('cat_lr', 0.005, 0.05, log=True),
                'depth': trial.suggest_int('cat_depth', 4, 10),
                'l2_leaf_reg': trial.suggest_float('cat_l2', 0.1, 10.0, log=True),
                'random_strength': trial.suggest_float('cat_random_strength', 0.1, 10.0, log=True),
                'bagging_temperature': trial.suggest_float('cat_bagging_temp', 0.0, 10.0),
                'random_seed': 42,
                'verbose': 0
            }

            # 딥러닝 모델 하이퍼파라미터
            dl_params = {
                'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),
                'lr': trial.suggest_float('dl_lr', 1e-5, 1e-3, log=True),
                'weight_decay': trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True),
                'dropout': trial.suggest_float('dropout', 0.1, 0.5),
                'transformer_heads': trial.suggest_int('transformer_heads', 2, 8),
                'transformer_layers': trial.suggest_int('transformer_layers', 1, 4),
            }

            # 앙상블 가중치
            weights = {
                'transformer_weight': trial.suggest_float('transformer_weight', 0.1, 0.5),
                'cnn_weight': trial.suggest_float('cnn_weight', 0.1, 0.5),
                'gru_weight': trial.suggest_float('gru_weight', 0.1, 0.5),
                'lgbm_weight': trial.suggest_float('lgbm_weight', 0.1, 0.5),
                'xgb_weight': trial.suggest_float('xgb_weight', 0.1, 0.5),
                'cat_weight': trial.suggest_float('cat_weight', 0.1, 0.5),
            }

            # 가중치 정규화
            weight_sum = sum(weights.values())
            for k in weights:
                weights[k] /= weight_sum

            # 모델 학습
            lgbm = LGBMClassifier(**lgbm_params)
            xgb = XGBClassifier(**xgb_params)
            cat = CatBoostClassifier(**cat_params)

            lgbm.fit(X_train, y_train)
            xgb.fit(X_train, y_train)
            cat.fit(X_train, y_train)

            # 예측
            lgbm_pred = lgbm.predict_proba(X_val)[:, 1]
            xgb_pred = xgb.predict_proba(X_val)[:, 1]
            cat_pred = cat.predict_proba(X_val)[:, 1]

            # 간소화된 딥러닝 모델 예측 (Optuna 실행 속도를 위해)
            # 실제 구현에서는 train_model 함수를 호출하여 딥러닝 모델을 학습시킬 수 있음
            transformer_pred = np.random.rand(len(y_val))
            cnn_pred = np.random.rand(len(y_val))
            gru_pred = np.random.rand(len(y_val))

            # 가중 앙상블
            ensemble_pred = (
                weights['transformer_weight'] * transformer_pred +
                weights['cnn_weight'] * cnn_pred +
                weights['gru_weight'] * gru_pred +
                weights['lgbm_weight'] * lgbm_pred +
                weights['xgb_weight'] * xgb_pred +
                weights['cat_weight'] * cat_pred
            )

            # AUC 계산
            auc = roc_auc_score(y_val, ensemble_pred)
            return auc

        # Optuna 연구 생성 및 실행
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=20)

        # 최적 하이퍼파라미터 및 가중치 반환
        return study.best_params, study.best_value

    def train(self, X, y, batch_size=32, n_splits=5):
        """교차 검증을 통한 앙상블 모델 학습"""
        X_pca, X_scaled = self.prepare_data(X, y, is_training=True)

        # K-fold 교차 검증 설정
        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
        fold_results = []
        all_val_preds = np.zeros(len(X_pca))

        # 하이퍼파라미터 최적화 (선택적)
        if self.use_optuna:
            print("하이퍼파라미터 최적화 시작...")
            # 최적화를 위한 훈련/검증 데이터 분할
            train_idx, val_idx = next(kf.split(X_pca, y))
            X_train_pca, X_val_pca = X_pca[train_idx], X_pca[val_idx]
            X_train_scaled, X_val_scaled = X_scaled[train_idx], X_scaled[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            best_params, best_score = self.optimize_hyperparams(
                X_train_scaled, y_train, X_val_scaled, y_val
            )
            print(f"최적 하이퍼파라미터 발견: {best_params}")
            print(f"최적 검증 AUC: {best_score:.4f}")

            # 최적 가중치 저장
            self.best_weights = {
                'transformer_weight': best_params['transformer_weight'],
                'cnn_weight': best_params['cnn_weight'],
                'gru_weight': best_params['gru_weight'],
                'lgbm_weight': best_params['lgbm_weight'],
                'xgb_weight': best_params['xgb_weight'],
                'cat_weight': best_params['cat_weight'],
            }

            # 배치 크기 업데이트
            batch_size = best_params['batch_size']
        else:
            # 기본 가중치 설정
            self.best_weights = {
                'transformer_weight': 0.20,
                'cnn_weight': 0.20,
                'gru_weight': 0.15,
                'lgbm_weight': 0.15,
                'xgb_weight': 0.15,
                'cat_weight': 0.15
            }

        for fold, (train_idx, val_idx) in enumerate(kf.split(X_pca, y)):
            print(f"\n=== Fold {fold+1}/{n_splits} ===")

            # 데이터 분할
            X_train_pca, X_val_pca = X_pca[train_idx], X_pca[val_idx]
            X_train_scaled, X_val_scaled = X_scaled[train_idx], X_scaled[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            # 클래스 가중치 계산 (불균형 문제 해결)
            class_weights = compute_sample_weight('balanced', y_train)
            sample_weights = torch.FloatTensor(class_weights)

            # 가중치가 적용된 데이터셋 생성
            train_dataset = FertilityDataset(X_train_pca, y_train, class_weights)
            val_dataset = FertilityDataset(X_val_pca, y_val)

            # 가중치 기반 샘플러 생성
            sampler = WeightedRandomSampler(
                weights=sample_weights,
                num_samples=len(sample_weights),
                replacement=True
            )

            # 데이터 로더 설정
            train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)
            val_loader = DataLoader(val_dataset, batch_size=batch_size)

            # Transformer 모델 학습
            if self.use_optuna:
                transformer = EnhancedTransformerModel(
                    X_train_pca.shape[1],
                    num_heads=best_params['transformer_heads'],
                    num_layers=best_params['transformer_layers'],
                    dropout=best_params['dropout']
                ).to(self.device)
                optimizer = torch.optim.AdamW(
                    transformer.parameters(),
                    lr=best_params['dl_lr'],
                    weight_decay=best_params['weight_decay']
                )
            else:
                transformer = EnhancedTransformerModel(X_train_pca.shape[1]).to(self.device)
                optimizer = torch.optim.AdamW(transformer.parameters(), lr=1e-4, weight_decay=0.01)

            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode='min', factor=0.5, patience=5
            )
            criterion = nn.BCELoss()

            transformer = train_model(
                transformer, train_loader, val_loader, criterion, optimizer, self.device,
                epochs=50, scheduler=scheduler
            )

            # CNN 모델 학습
            if self.use_optuna:
                cnn = EnhancedCNNModel(
                    X_train_pca.shape[1],
                    dropout=best_params['dropout']
                ).to(self.device)
                optimizer = torch.optim.AdamW(
                    cnn.parameters(),
                    lr=best_params['dl_lr'],
                    weight_decay=best_params['weight_decay']
                )
            else:
                cnn = EnhancedCNNModel(X_train_pca.shape[1]).to(self.device)
                optimizer = torch.optim.AdamW(cnn.parameters(), lr=1e-4, weight_decay=0.01)

            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode='min', factor=0.5, patience=5
            )

            cnn = train_model(
                cnn, train_loader, val_loader, criterion, optimizer, self.device,
                epochs=50, scheduler=scheduler
            )

            # GRU 모델 학습 (새로 추가)
            if self.use_optuna:
                gru = GRUModel(
                    X_train_pca.shape[1],
                    hidden_dim=128,
                    num_layers=2,
                    dropout=best_params['dropout']
                ).to(self.device)
                optimizer = torch.optim.AdamW(
                    gru.parameters(),
                    lr=best_params['dl_lr'],
                    weight_decay=best_params['weight_decay']
                )
            else:
                gru = GRUModel(X_train_pca.shape[1]).to(self.device)
                optimizer = torch.optim.AdamW(gru.parameters(), lr=1e-4, weight_decay=0.01)

            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode='min', factor=0.5, patience=5
            )

            gru = train_model(
                gru, train_loader, val_loader, criterion, optimizer, self.device,
                epochs=50, scheduler=scheduler
            )

            # LightGBM 모델 학습
            if self.use_optuna:
                lgbm_params = {k: v for k, v in best_params.items() if k.startswith('lgbm_')}
                # 접두사 제거
                lgbm_params = {k.replace('lgbm_', ''): v for k, v in lgbm_params.items()}
                lgbm = LGBMClassifier(**lgbm_params)
            else:
                lgbm = LGBMClassifier(
                    n_estimators=2000,
                    learning_rate=0.01,
                    num_leaves=31,
                    max_depth=7,
                    min_child_samples=20,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    class_weight='balanced',
                    random_state=42
                )
            lgbm.fit(
                X_train_scaled, y_train,
                eval_set=[(X_val_scaled, y_val)],
                eval_metric='auc',
                verbose=100
            )

            # XGBoost 모델 학습
            if self.use_optuna:
                xgb_params = {k: v for k, v in best_params.items() if k.startswith('xgb_')}
                # 접두사 제거
                xgb_params = {k.replace('xgb_', ''): v for k, v in xgb_params.items()}
                # 필수 파라미터 추가
                xgb_params['use_label_encoder'] = False
                xgb_params['eval_metric'] = 'logloss'
                xgb = XGBClassifier(**xgb_params)
            else:
                xgb = XGBClassifier(
                    n_estimators=2000,
                    learning_rate=0.01,
                    max_depth=6,
                    min_child_weight=1,
                    gamma=0,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    scale_pos_weight=sum(y_train==0)/sum(y_train==1),  # 클래스 불균형 처리
                    random_state=42,
                    use_label_encoder=False,
                    eval_metric='logloss'
                )

            xgb.fit(
                X_train_scaled, y_train,
                eval_set=[(X_val_scaled, y_val)],
                verbose=100
            )

            # CatBoost 모델 학습 (새로 추가)
            if self.use_optuna:
                cat_params = {k: v for k, v in best_params.items() if k.startswith('cat_')}
                # 접두사 제거
                cat_params = {k.replace('cat_', ''): v for k, v in cat_params.items()}
                cat = CatBoostClassifier(**cat_params)
            else:
                cat = CatBoostClassifier(
                    iterations=1000,
                    learning_rate=0.02,
                    depth=6,
                    l2_leaf_reg=3,
                    random_strength=1,
                    bagging_temperature=1,
                    auto_class_weights='Balanced',
                    random_seed=42,
                    verbose=100
                )

            cat.fit(
                X_train_scaled, y_train,
                eval_set=(X_val_scaled, y_val),
                verbose=0
            )

            # 검증 세트에 대한 예측
            transformer.eval()
            cnn.eval()
            gru.eval()

            with torch.no_grad():
                val_tensor = torch.FloatTensor(X_val_pca).to(self.device)
                transformer_preds = transformer(val_tensor).cpu().numpy().ravel()
                cnn_preds = cnn(val_tensor).cpu().numpy().ravel()
                gru_preds = gru(val_tensor).cpu().numpy().ravel()

            lgbm_preds = lgbm.predict_proba(X_val_scaled)[:, 1]
            xgb_preds = xgb.predict_proba(X_val_scaled)[:, 1]
            cat_preds = cat.predict_proba(X_val_scaled)[:, 1]

            # 가중 앙상블 예측
            ensemble_preds = (
                self.best_weights['transformer_weight'] * transformer_preds +
                self.best_weights['cnn_weight'] * cnn_preds +
                self.best_weights['gru_weight'] * gru_preds +
                self.best_weights['lgbm_weight'] * lgbm_preds +
                self.best_weights['xgb_weight'] * xgb_preds +
                self.best_weights['cat_weight'] * cat_preds
            )

            # 현재 폴드의 예측값 저장
            all_val_preds[val_idx] = ensemble_preds

            # 최종 예측 및 메트릭 계산
            final_preds = (ensemble_preds > 0.5).astype(int)

            # 메트릭 계산
            accuracy = accuracy_score(y_val, final_preds)
            auc = roc_auc_score(y_val, ensemble_preds)
            f1 = f1_score(y_val, final_preds)

            fold_results.append({
                'fold': fold + 1,
                'accuracy': accuracy,
                'auc': auc,
                'f1': f1
            })

            # 각 모델의 개별 성능 평가
            transformer_score = roc_auc_score(y_val, transformer_preds)
            cnn_score = roc_auc_score(y_val, cnn_preds)
            gru_score = roc_auc_score(y_val, gru_preds)
            lgbm_score = roc_auc_score(y_val, lgbm_preds)
            xgb_score = roc_auc_score(y_val, xgb_preds)
            cat_score = roc_auc_score(y_val, cat_preds)

            print(f"\nFold {fold+1} 결과:")
            print(f"Transformer AUC: {transformer_score:.4f}")
            print(f"CNN AUC: {cnn_score:.4f}")
            print(f"GRU AUC: {gru_score:.4f}")
            print(f"LightGBM AUC: {lgbm_score:.4f}")
            print(f"XGBoost AUC: {xgb_score:.4f}")
            print(f"CatBoost AUC: {cat_score:.4f}")
            print(f"앙상블 AUC: {auc:.4f}")
            print(f"앙상블 Accuracy: {accuracy:.4f}")
            print(f"앙상블 F1: {f1:.4f}")

            # 마지막 폴드의 모델 저장
            if fold == n_splits - 1:
                self.transformer = transformer
                self.cnn = cnn
                self.gru = gru
                self.lgbm_classifier = lgbm
                self.xgb_classifier = xgb
                self.catboost_classifier = cat

        # 전체 교차 검증 결과 계산
        avg_accuracy = np.mean([r['accuracy'] for r in fold_results])
        avg_auc = np.mean([r['auc'] for r in fold_results])
        avg_f1 = np.mean([r['f1'] for r in fold_results])

        print("\n=== 교차 검증 결과 요약 ===")
        print(f"평균 Accuracy: {avg_accuracy:.4f}")
        print(f"평균 AUC: {avg_auc:.4f}")
        print(f"평균 F1: {avg_f1:.4f}")

        # 전체 데이터에 대한 Out-of-fold 예측 AUC
        oof_auc = roc_auc_score(y, all_val_preds)
        print(f"Out-of-fold AUC: {oof_auc:.4f}")

        return {
            'accuracy': avg_accuracy,
            'auc': avg_auc,
            'f1': avg_f1,
            'oof_auc': oof_auc,
            'fold_results': fold_results,
            'oof_predictions': all_val_preds
        }

    def predict(self, X, batch_size=16):
        """배치 처리를 통한 예측"""
        X_pca, X_scaled = self.prepare_data(X, is_training=False)

        predictions = []
        # 큰 데이터셋에 대해 배치 처리
        for i in range(0, len(X_pca), batch_size):
            batch = torch.FloatTensor(X_pca[i:i+batch_size]).to(self.device)

            # 딥러닝 모델 예측
            self.transformer.eval()
            self.cnn.eval()
            self.gru.eval()

            with torch.no_grad():
                transformer_pred = self.transformer(batch).cpu().numpy().ravel()
                cnn_pred = self.cnn(batch).cpu().numpy().ravel()
                gru_pred = self.gru(batch).cpu().numpy().ravel()

            # Tree 기반 모델 예측
            lgbm_pred = self.lgbm_classifier.predict_proba(X_scaled[i:i+batch_size])[:, 1]
            xgb_pred = self.xgb_classifier.predict_proba(X_scaled[i:i+batch_size])[:, 1]
            cat_pred = self.catboost_classifier.predict_proba(X_scaled[i:i+batch_size])[:, 1]

            # 저장된 최적 가중치를 사용한 앙상블 예측
            ensemble_pred = (
                self.best_weights['transformer_weight'] * transformer_pred +
                self.best_weights['cnn_weight'] * cnn_pred +
                self.best_weights['gru_weight'] * gru_pred +
                self.best_weights['lgbm_weight'] * lgbm_pred +
                self.best_weights['xgb_weight'] * xgb_pred +
                self.best_weights['cat_weight'] * cat_pred
            )
            predictions.append(ensemble_pred)

        # 모든 배치의 예측을 합침
        all_predictions = np.concatenate(predictions)
        return (all_predictions > 0.5).astype(int), all_predictions

    def predict_with_confidence(self, X, batch_size=16, n_bootstrap=10):
        """부트스트랩 샘플링을 통한 예측 신뢰도 추정"""
        X_pca, X_scaled = self.prepare_data(X, is_training=False)

        all_bootstrap_preds = []
        # 부트스트랩 반복
        for b in range(n_bootstrap):
            bootstrap_indices = np.random.choice(len(X_pca), size=len(X_pca), replace=True)
            X_bootstrap = X_pca[bootstrap_indices]
            X_scaled_bootstrap = X_scaled[bootstrap_indices]

            batch_preds = []
            # 배치 처리
            for i in range(0, len(X_bootstrap), batch_size):
                batch = torch.FloatTensor(X_bootstrap[i:i+batch_size]).to(self.device)

                # 딥러닝 모델 예측
                self.transformer.eval()
                self.cnn.eval()
                self.gru.eval()

                with torch.no_grad():
                    transformer_pred = self.transformer(batch).cpu().numpy().ravel()
                    cnn_pred = self.cnn(batch).cpu().numpy().ravel()
                    gru_pred = self.gru(batch).cpu().numpy().ravel()

                # Tree 기반 모델 예측
                lgbm_pred = self.lgbm_classifier.predict_proba(X_scaled_bootstrap[i:i+batch_size])[:, 1]
                xgb_pred = self.xgb_classifier.predict_proba(X_scaled_bootstrap[i:i+batch_size])[:, 1]
                cat_pred = self.catboost_classifier.predict_proba(X_scaled_bootstrap[i:i+batch_size])[:, 1]

                # 가중 앙상블 예측
                ensemble_pred = (
                    self.best_weights['transformer_weight'] * transformer_pred +
                    self.best_weights['cnn_weight'] * cnn_pred +
                    self.best_weights['gru_weight'] * gru_pred +
                    self.best_weights['lgbm_weight'] * lgbm_pred +
                    self.best_weights['xgb_weight'] * xgb_pred +
                    self.best_weights['cat_weight'] * cat_pred
                )
                batch_preds.append(ensemble_pred)

            # 현재 부트스트랩의 모든 배치 예측 합치기
            bootstrap_preds = np.concatenate(batch_preds)
            # 원래 인덱스로 재정렬
            bootstrap_preds_reindexed = np.zeros(len(X_pca))
            bootstrap_preds_reindexed[bootstrap_indices] = bootstrap_preds
            all_bootstrap_preds.append(bootstrap_preds_reindexed)

        # 부트스트랩 예측 스택
        all_bootstrap_preds = np.stack(all_bootstrap_preds, axis=0)

        # 평균 예측 및 표준 편차 계산
        mean_preds = np.mean(all_bootstrap_preds, axis=0)
        std_preds = np.std(all_bootstrap_preds, axis=0)

        # 신뢰구간 계산 (95%)
        lower_ci = np.percentile(all_bootstrap_preds, 2.5, axis=0)
        upper_ci = np.percentile(all_bootstrap_preds, 97.5, axis=0)

        return {
            'predictions': (mean_preds > 0.5).astype(int),
            'probabilities': mean_preds,
            'std_dev': std_preds,
            'lower_ci': lower_ci,
            'upper_ci': upper_ci
        }
# Main execution
if __name__ == "__main__":
    # 데이터 로드
    train_data = pd.read_csv('/content/drive/MyDrive/lg aimers/transformed_train_df.csv')
    test_data = pd.read_csv('/content/drive/MyDrive/lg aimers/transformed_test_df.csv')
    sample_submission = pd.read_csv('/content/drive/MyDrive/lg aimers/sample_submission.csv')

    # 중복 열 확인 및 제거
    train_dup_cols = train_data.columns[train_data.columns.duplicated()].tolist()
    if train_dup_cols:
        print(f"훈련 데이터에서 중복된 열 발견: {train_dup_cols}")
        train_data = train_data.loc[:, ~train_data.columns.duplicated(keep='first')]

    test_dup_cols = test_data.columns[test_data.columns.duplicated()].tolist()
    if test_dup_cols:
        print(f"테스트 데이터에서 중복된 열 발견: {test_dup_cols}")
        test_data = test_data.loc[:, ~test_data.columns.duplicated(keep='first')]

    # DataFrame 최적화 (defragmentation)
    train_data = train_data.copy()
    test_data = test_data.copy()

    # 데이터 탐색 및 클래스 불균형 확인
    print("훈련 데이터 정보:")
    print(train_data.info())
    print("\n클래스 분포:")
    y_distribution = train_data['임신 성공 여부'].value_counts(normalize=True)
    print(y_distribution)

    # 특성과 타겟 분리
    X = train_data.drop('임신 성공 여부', axis=1)
    y = train_data['임신 성공 여부'].values

    # 모델 학습 시작
    print("모델 학습 시작...")
    ensemble = EnhancedFertilityEnsemble(n_components=150, use_optuna=False)  # Optuna는 필요에 따라 활성화
    results = ensemble.train(X, y, batch_size=32, n_splits=5)

    # 테스트 데이터에 대한 예측
    print("\n테스트 데이터 예측 중...")
    predictions, probabilities = ensemble.predict(test_data)

    # 결과 저장
    sample_submission['probability'] = probabilities

    # 제출 파일 저장
    sample_submission.to_csv('/content/drive/MyDrive/lg aimers/enhanced_ensemble_submission.csv', index=False)

    # 결과 요약 출력
    print("\n훈련 결과 요약:")
    print(f"Cross-Validation 평균 Accuracy: {results['accuracy']:.4f}")
    print(f"Cross-Validation 평균 AUC: {results['auc']:.4f}")
    print(f"Cross-Validation 평균 F1 점수: {results['f1']:.4f}")
    print(f"Out-of-fold AUC: {results['oof_auc']:.4f}")

    print("\n예측 분포:")
    print(sample_submission['probability'].describe())