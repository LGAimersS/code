# -*- coding: utf-8 -*-
"""AI_solution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zFMz-mU4vbwU2B91IcFMYbmVX9Pe5tFU
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from lightgbm import LGBMClassifier  # 변경됨: lgbm → LGBMClassifier
from xgboost import XGBClassifier    # 변경됨: xgb → XGBClassifier
from scipy.stats import mode

from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_selection import VarianceThreshold
from sklearn.utils.class_weight import compute_sample_weight
from torch.utils.data import WeightedRandomSampler

import os
os.environ["LOKY_MAX_CPU_COUNT"] = "4"
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# Custom Dataset for PyTorch
class FertilityDataset(Dataset):
    def __init__(self, features, targets=None):
        if isinstance(features, (pd.DataFrame, pd.Series)):
            features = features.values
        self.features = torch.FloatTensor(features)

        if targets is not None:
            if isinstance(targets, pd.Series):
                targets = targets.values
            self.targets = torch.FloatTensor(targets)
        else:
            self.targets = None

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        if self.targets is not None:
            return self.features[idx], self.targets[idx]
        return self.features[idx]

# Training function for deep learning models
def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=20, scheduler=None):
    for epoch in range(epochs):
        # Training Phase
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for features, targets in train_loader:
            features, targets = features.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, targets.view(-1, 1))
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = (outputs.data > 0.5).float()
            train_total += targets.size(0)
            train_correct += (predicted.view(-1) == targets).sum().item()

        avg_train_loss = train_loss / len(train_loader)
        train_accuracy = 100 * train_correct / train_total

        # Validation Phase
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for features, targets in val_loader:
                features, targets = features.to(device), targets.to(device)
                outputs = model(features)
                loss = criterion(outputs, targets.view(-1, 1))

                val_loss += loss.item()
                predicted = (outputs.data > 0.5).float()
                val_total += targets.size(0)
                val_correct += (predicted.view(-1) == targets).sum().item()

        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = 100 * val_correct / val_total

        if scheduler is not None:
            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                scheduler.step(avg_val_loss)
            else:
                scheduler.step()

        print(f'Epoch [{epoch+1}/{epochs}]')
        print(f'Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')
        print(f'Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')
        print('-' * 60)

    return model

# Neural Network Models
# Improved Transformer Model
class EnhancedTransformerModel(nn.Module):
    def __init__(self, input_dim, num_heads=4, num_layers=2, dropout=0.2):  # num_heads와 num_layers 감소
        super().__init__()
        self.embedding1 = nn.Linear(input_dim, 128)  # 256에서 128로 감소
        self.embedding2 = nn.Linear(input_dim, 64)   # 128에서 64로 감소

        encoder_layer1 = nn.TransformerEncoderLayer(
            d_model=128,  # 256에서 128로 감소
            nhead=num_heads,
            dropout=dropout,
            activation='gelu'
        )
        encoder_layer2 = nn.TransformerEncoderLayer(
            d_model=64,   # 128에서 64로 감소
            nhead=num_heads//2,
            dropout=dropout,
            activation='gelu'
        )

        self.transformer1 = nn.TransformerEncoder(encoder_layer1, num_layers)
        self.transformer2 = nn.TransformerEncoder(encoder_layer2, num_layers)

        self.fc = nn.Sequential(
            nn.Linear(192, 96),    # 384에서 192로 감소
            nn.SELU(),
            nn.Dropout(dropout),
            nn.Linear(96, 48),     # 192에서 96으로 감소
            nn.SELU(),
            nn.Dropout(dropout),
            nn.Linear(48, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x1 = self.embedding1(x).unsqueeze(1)
        x2 = self.embedding2(x).unsqueeze(1)

        x1 = self.transformer1(x1).squeeze(1)
        x2 = self.transformer2(x2).squeeze(1)

        x_combined = torch.cat([x1, x2], dim=1)
        return self.fc(x_combined)

# Improved CNN Model
class EnhancedCNNModel(nn.Module):
    def __init__(self, input_dim):
        super().__init__()

        def conv_block(in_channels, out_channels):
            return nn.Sequential(
                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),
                nn.BatchNorm1d(out_channels),
                nn.SELU(),
                nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),
                nn.BatchNorm1d(out_channels),
                nn.SELU(),
            )

        self.proj1 = nn.Conv1d(16, 32, kernel_size=1)  # 32->16, 64->32
        self.proj2 = nn.Conv1d(32, 64, kernel_size=1)  # 64->32, 128->64
        self.input_proj = nn.Conv1d(1, 16, kernel_size=1)  # 32->16
        self.conv1 = conv_block(16, 32)  # 32->16, 64->32
        self.conv2 = conv_block(32, 64)  # 64->32, 128->64
        self.conv3 = conv_block(64, 128)  # 128->64, 256->128
        self.pool = nn.AdaptiveAvgPool1d(8)  # 16에서 8로 감소

        self.fc = nn.Sequential(
            nn.Linear(128 * 8, 256),  # 256*16 -> 128*8
            nn.SELU(),
            nn.Dropout(0.3),
            nn.Linear(256, 64),
            nn.SELU(),
            nn.Dropout(0.2),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.input_proj(x)

        identity = x
        x = self.conv1(x)
        identity = self.proj1(identity)
        x = x + identity

        identity = x
        x = self.conv2(x)
        identity = self.proj2(identity)
        x = x + identity

        x = self.conv3(x)
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        return self.fc(x)

class EnhancedFertilityEnsemble:
    def __init__(self, n_components=150):
        self.n_components = n_components
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=n_components)
        self.transformer = None
        self.cnn = None
        self.lgbm_classifier = None  # 변경됨: self.lgbm → self.lgbm_classifier
        self.xgb_classifier = None   # 변경됨: self.xgb → self.xgb_classifier
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    def prepare_data(self, X, is_training=True):
      if 'ID' in X.columns:
          X = X.drop('ID', axis=1)
      elif 'id' in X.columns:
          X = X.drop('id', axis=1)

      if isinstance(X, pd.DataFrame):
          numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns

          if is_training:
              # 학습 시에만 fit_transform 사용
              self.poly = PolynomialFeatures(degree=2, include_bias=False)
              X_poly = self.poly.fit_transform(X[numeric_cols])

              # feature names 생성
              self.poly_feature_names = self.poly.get_feature_names_out(numeric_cols)
              X_poly_df = pd.DataFrame(X_poly, columns=self.poly_feature_names)

              self.selector = VarianceThreshold(threshold=0.01)
              X_poly_selected = self.selector.fit_transform(X_poly_df)

              # 선택된 feature names 저장
              self.selected_features = self.poly_feature_names[self.selector.get_support()]

          else:
            # 예측 시에는 저장된 변환 사용
              X_poly = self.poly.transform(X[numeric_cols])
              X_poly_df = pd.DataFrame(X_poly, columns=self.poly_feature_names)
              X_poly_selected = self.selector.transform(X_poly_df)

        # 선택된 특성들로 DataFrame 생성
          X_poly_df = pd.DataFrame(X_poly_selected, columns=self.selected_features)
          X = pd.concat([X, X_poly_df], axis=1)
          X = X.values

      if is_training:
          X_scaled = self.scaler.fit_transform(X)
          X_pca = self.pca.fit_transform(X_scaled)
      else:
          X_scaled = self.scaler.transform(X)
          X_pca = self.pca.transform(X_scaled)

      return X_pca, X_scaled

    def train(self, X, y, batch_size=32):
        X_pca, X_scaled = self.prepare_data(X, is_training=True)

        # Train/Validation split (80-20)
        train_size = int(0.8 * len(X))

        # Indices for splitting
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        train_idx, val_idx = indices[:train_size], indices[train_size:]

        # Split data
        X_train_pca, X_val_pca = X_pca[train_idx], X_pca[val_idx]
        X_train_scaled, X_val_scaled = X_scaled[train_idx], X_scaled[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        # Prepare data loaders
        train_dataset = FertilityDataset(X_train_pca, y_train)
        val_dataset = FertilityDataset(X_val_pca, y_val)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size)

        # Train Transformer
        self.transformer = EnhancedTransformerModel(self.n_components).to(self.device)
        optimizer = torch.optim.AdamW(self.transformer.parameters(), lr=1e-4, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
        criterion = nn.BCELoss()
        self.transformer = train_model(
            self.transformer, train_loader, val_loader, criterion, optimizer, self.device,
            epochs=20, scheduler=scheduler
        )

        # Train CNN
        self.cnn = EnhancedCNNModel(self.n_components).to(self.device)
        optimizer = torch.optim.AdamW(self.cnn.parameters(), lr=1e-4, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
        self.cnn = train_model(
            self.cnn, train_loader, val_loader, criterion, optimizer, self.device,
            epochs=20, scheduler=scheduler
        )

        # Train LightGBM
        self.lgbm_classifier = LGBMClassifier(  # 변경됨: self.lgbm → self.lgbm_classifier
            n_estimators=1000,
            learning_rate=0.01,
            num_leaves=31,
            max_depth=7,
            min_child_samples=20,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42
        )
        self.lgbm_classifier.fit(X_train_scaled, y_train)  # 변경됨: self.lgbm → self.lgbm_classifier

        # Train XGBoost
        self.xgb_classifier = XGBClassifier(  # 변경됨: self.xgb → self.xgb_classifier
            n_estimators=1000,
            learning_rate=0.01,
            max_depth=6,
            min_child_weight=1,
            gamma=0,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss'
        )
        self.xgb_classifier.fit(X_train_scaled, y_train)  # 변경됨: self.xgb → self.xgb_classifier

        # Make predictions for validation set
        self.transformer.eval()
        self.cnn.eval()
        with torch.no_grad():
            val_tensor = torch.FloatTensor(X_val_pca).to(self.device)
            transformer_preds = self.transformer(val_tensor).cpu().numpy().ravel()
            cnn_preds = self.cnn(val_tensor).cpu().numpy().ravel()

        lgbm_preds = self.lgbm_classifier.predict_proba(X_val_scaled)[:, 1]  # 변경됨: self.lgbm → self.lgbm_classifier
        xgb_preds = self.xgb_classifier.predict_proba(X_val_scaled)[:, 1]    # 변경됨: self.xgb → self.xgb_classifier

        # Weighted ensemble predictions
        ensemble_preds = (
            0.3 * transformer_preds +
            0.3 * cnn_preds +
            0.2 * lgbm_preds +
            0.2 * xgb_preds
        )
        final_preds = (ensemble_preds > 0.5).astype(int)

        # Calculate metrics
        accuracy = accuracy_score(y_val, final_preds)
        auc = roc_auc_score(y_val, ensemble_preds)

        return {
            'accuracy': accuracy,
            'auc': auc,
            'predictions': final_preds,
            'probabilities': ensemble_preds
        }

    def predict(self, X):
      X_pca, X_scaled = self.prepare_data(X, is_training=False)
      batch_size = 16  # 또는 더 작은 값으로 설정

      predictions = []
      for i in range(0, len(X_pca), batch_size):
        batch = torch.FloatTensor(X_pca[i:i+batch_size]).to(self.device)

        self.transformer.eval()
        self.cnn.eval()
        with torch.no_grad():
            transformer_pred = self.transformer(batch).cpu().numpy().ravel()
            cnn_pred = self.cnn(batch).cpu().numpy().ravel()

        lgbm_pred = self.lgbm_classifier.predict_proba(X_scaled[i:i+batch_size])[:, 1]  # 변경됨: self.lgbm → self.lgbm_classifier
        xgb_pred = self.xgb_classifier.predict_proba(X_scaled[i:i+batch_size])[:, 1]    # 변경됨: self.xgb → self.xgb_classifier

        # Fixed weights for ensemble prediction
        ensemble_pred = (
            0.3 * transformer_pred +
            0.3 * cnn_pred +
            0.2 * lgbm_pred +
            0.2 * xgb_pred
        )
        predictions.append(ensemble_pred)

      # 모든 배치의 예측을 합침
      all_predictions = np.concatenate(predictions)
      return (all_predictions > 0.5).astype(int), all_predictions

# Main execution
if __name__ == "__main__":
    # Load data
    train_data = pd.read_csv('/content/drive/MyDrive/lg aimers/transformed_train_df.csv')
    test_data = pd.read_csv('/content/drive/MyDrive/lg aimers/transformed_test_df.csv')
    sample_submission = pd.read_csv('/content/drive/MyDrive/lg aimers/sample_submission.csv')

    X = train_data.drop('임신 성공 여부', axis=1)
    y = train_data['임신 성공 여부']

    # Train model
    ensemble = EnhancedFertilityEnsemble(n_components=100)
    results = ensemble.train(X, y, batch_size=16)

    # Make predictions on test data
    predictions, probabilities = ensemble.predict(test_data)

    # Save predictions
    sample_submission['probability'] = probabilities
    sample_submission.to_csv('/content/drive/MyDrive/lg aimers/ensemble_submission.csv', index=False)

    # Print results
    print("\nTraining Results:")
    print(f"Accuracy: {results['accuracy']:.4f}")
    print(f"AUC: {results['auc']:.4f}")
    print("\nPrediction Distribution:")
    print(sample_submission['probability'].describe())