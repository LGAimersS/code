# -*- coding: utf-8 -*-
"""correlation_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZbIJ8kdwpsCKjNu70RlmNiPz8YPdL1LC
"""

from google.colab import drive
drive.mount('/content/drive')

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

"""###Data Load"""

import pandas as pd
import numpy as np

class DataTransformer:
    def __init__(self, train_data, target_column='임신 성공 여부', exclude_cols=None, unknown_values=None):
        self.target_column = target_column
        self.exclude_cols = exclude_cols or []
        self.unknown_values = unknown_values or ['알 수 없음', 'Unknown', 'NA', 'N/A']
        self.transformation_details = {}
        self.bin_edges_dict, self.value_mapping_dict, self.bin_probabilities = self._generate_transformation_info(train_data)

    def _replace_unknown_values(self, data):
        data = data.copy()
        for column in data.columns:
            if column not in self.exclude_cols and column != self.target_column:
                mask = data[column].astype(str).str.lower().isin([str(val).lower() for val in self.unknown_values])
                data.loc[mask, column] = np.nan
        return data

    def _calculate_probability_based_bins(self, series, target_series, n_bins=5):
        valid_mask = series.notna() & target_series.notna()
        valid_series = series[valid_mask]
        valid_target = target_series[valid_mask]

        if len(valid_series) == 0:
            return None, None

        # 초기 구간 생성
        percentiles = np.linspace(0, 100, n_bins + 1)
        bin_edges = np.percentile(valid_series, percentiles)
        bin_edges[0] = -np.inf
        bin_edges[-1] = np.inf

        # 각 구간의 타겟 비율 계산
        bins = pd.cut(valid_series, bins=bin_edges, duplicates='drop')
        bin_probabilities = valid_target.groupby(bins).mean()

        return bin_edges, bin_probabilities

    def _generate_transformation_info(self, train_data):
        train_data = self._replace_unknown_values(train_data)

        bin_edges_dict = {}
        value_mapping_dict = {}
        bin_probabilities_dict = {}

        for column in train_data.columns:
            if column in self.exclude_cols or column == self.target_column:
                continue

            if pd.api.types.is_numeric_dtype(train_data[column]):
                bin_edges, bin_probabilities = self._calculate_probability_based_bins(
                    train_data[column],
                    train_data[self.target_column]
                )

                if bin_edges is not None:
                    bin_edges_dict[column] = bin_edges
                    bin_probabilities_dict[column] = bin_probabilities

                    # 확률에 따라 0~1 사이 값으로 매핑
                    sorted_probs = bin_probabilities.sort_values(ascending=False)
                    value_map = {}
                    for i, (bin_range, prob) in enumerate(sorted_probs.items()):
                        value_map[bin_range] = round((len(sorted_probs) - i - 1) / (len(sorted_probs) - 1), 2)

                    value_mapping_dict[column] = value_map
            else:
                category_probs = train_data[self.target_column].groupby(train_data[column]).mean()
                sorted_categories = category_probs.sort_values(ascending=False)

                # 카테고리별 확률을 5개 구간으로 나누어 매핑
                n_categories = len(sorted_categories)
                value_map = {}
                for i, (cat, prob) in enumerate(sorted_categories.items()):
                    value_map[cat] = round((n_categories - i - 1) / (n_categories - 1 if n_categories > 1 else 1), 2)

                value_mapping_dict[column] = value_map
                bin_probabilities_dict[column] = sorted_categories

        return bin_edges_dict, value_mapping_dict, bin_probabilities_dict

    def transform_data(self, data):
        result_data = self._replace_unknown_values(data)
        transformation_details = {}

        for column in result_data.columns:
            if column in self.exclude_cols or column == self.target_column:
                continue

            original_values = result_data[column].value_counts(dropna=False).to_dict()

            if pd.api.types.is_numeric_dtype(result_data[column]):
                if column in self.bin_edges_dict:
                    bins = pd.cut(result_data[column], bins=self.bin_edges_dict[column], duplicates='drop')
                    result_data[column] = bins.map(self.value_mapping_dict[column]).fillna(np.nan)

                    # 구간별 상세 정보 저장
                    bin_details = {
                        'bin_edges': self.bin_edges_dict[column].tolist(),
                        'bin_probabilities': self.bin_probabilities[column].to_dict(),
                        'value_mapping': self.value_mapping_dict[column]
                    }
            else:
                result_data[column] = result_data[column].map(self.value_mapping_dict[column]).fillna(np.nan)
                bin_details = {
                    'category_probabilities': self.bin_probabilities[column].to_dict(),
                    'value_mapping': self.value_mapping_dict[column]
                }

            transformed_values = result_data[column].value_counts(dropna=False).to_dict()

            transformation_details[column] = {
                'original_values': original_values,
                'transformed_values': transformed_values,
                'details': bin_details
            }

        self.transformation_details = transformation_details
        return result_data

    def print_transformation_details(self):
        for column, details in self.transformation_details.items():
            print(f"\n{'='*50}")
            print(f"{column} 변환 상세:")
            print(f"{'='*50}")

            print("\n1. 원본 값 분포:")
            for val, count in details['original_values'].items():
                print(f"  {val}: {count}")

            print("\n2. 변환된 값 분포:")
            for val, count in details['transformed_values'].items():
                print(f"  {val}: {count}")

            print("\n3. 변환 상세 정보:")
            if 'bin_edges' in details['details']:
                print("  [수치형 변수]")
                edges = details['details']['bin_edges']
                probs = details['details']['bin_probabilities']
                mappings = details['details']['value_mapping']

                print("\n  구간별 정보:")
                for i in range(len(edges)-1):
                    bin_range = pd.Interval(edges[i], edges[i+1])
                    if bin_range in probs:
                        prob = probs[bin_range]
                        mapped_value = mappings.get(bin_range, "N/A")
                        print(f"  구간 {i+1}: {edges[i]} ~ {edges[i+1]}")
                        print(f"    - 성공 확률: {prob:.3f}")
                        print(f"    - 매핑된 값: {mapped_value}")
            else:
                print("  [범주형 변수]")
                probs = details['details']['category_probabilities']
                mappings = details['details']['value_mapping']

                print("\n  카테고리별 정보:")
                for cat, prob in probs.items():
                    mapped_value = mappings.get(cat, "N/A")
                    print(f"  {cat}:")
                    print(f"    - 성공 확률: {prob:.3f}")
                    print(f"    - 매핑된 값: {mapped_value}")


train_df = pd.read_csv('/content/drive/MyDrive/lg aimers/train.csv')
test_df = pd.read_csv('/content/drive/MyDrive/lg aimers/test.csv')

transformer = DataTransformer(train_df, target_column='임신 성공 여부', exclude_cols=['ID'])
transformed_train_df = transformer.transform_data(train_df)
transformed_test_df = transformer.transform_data(test_df)

transformer.print_transformation_details()

transformed_train_df

transformed_test_df

"""### NaN을 제외한 부분에서의 상관도를 확인하기"""

for column in transformed_train_df.columns:
   transformed_train_df[column] = pd.to_numeric(transformed_train_df[column], errors='coerce')
for column in transformed_test_df.columns:
   transformed_test_df[column] = pd.to_numeric(transformed_test_df[column], errors='coerce')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

def create_correlation_heatmap(df, exclude_cols=None, figsize=(40, 32)):
    """
    데이터프레임의 상관관계를 히트맵으로 시각화하는 함수
    NaN이 있는 경우 해당 값들을 제외하고 상관계수를 계산합니다

    Parameters:
    df (pandas.DataFrame): 입력 데이터프레임
    exclude_cols (list): 제외할 열 이름 리스트 (선택사항)
    figsize (tuple): 그래프 크기 (width, height)

    Returns:
    tuple: (상관계수 행렬, 데이터 커버리지 행렬)
    """
    plt.rc('font', family='NanumBarunGothic')

    # 분석할 열 선택
    if exclude_cols is None:
        exclude_cols = []

    numeric_cols = df.select_dtypes(include=[np.number]).columns
    columns_to_analyze = [col for col in numeric_cols if col not in exclude_cols]

    # 상관계수와 데이터 커버리지를 저장할 행렬 생성
    n_cols = len(columns_to_analyze)
    corr_matrix = pd.DataFrame(np.ones((n_cols, n_cols)),
                             columns=columns_to_analyze,
                             index=columns_to_analyze)
    coverage_matrix = pd.DataFrame(np.ones((n_cols, n_cols)),
                                 columns=columns_to_analyze,
                                 index=columns_to_analyze)

    # 각 열 쌍에 대해 상관계수와 데이터 커버리지 계산
    for i, col1 in enumerate(columns_to_analyze):
        for j, col2 in enumerate(columns_to_analyze):
            if i > j:  # 대각선 아래 부분만 계산 (대칭이므로)
                # 두 열에서 둘 다 값이 있는 행만 선택
                valid_data = df[[col1, col2]].dropna()

                # 데이터 커버리지 계산 (전체 행 대비 사용된 행의 비율)
                coverage = (len(valid_data) / len(df)) * 100

                # 상관계수 계산
                correlation = valid_data[col1].corr(valid_data[col2])

                # 행렬에 값 저장 (대칭적으로)
                corr_matrix.loc[col1, col2] = correlation
                corr_matrix.loc[col2, col1] = correlation
                coverage_matrix.loc[col1, col2] = coverage
                coverage_matrix.loc[col2, col1] = coverage

    # 히트맵 그리기
    plt.figure(figsize=figsize)
    plt.xticks(fontsize=18, rotation=45, ha='right')  # 라벨 45도 회전
    plt.yticks(fontsize=18)
    #plt.title('Correlation Heatmap\n(Numbers below correlation values show data coverage)',
    #          fontsize=14,
    #          pad=20)

    # 히트맵 생성
    mask = np.triu(np.ones_like(corr_matrix), k=0)  # 대각선과 위쪽을 마스킹
    sns.heatmap(corr_matrix,
                #mask=mask,
                annot=True,
                fmt='.2f',
                cmap='RdBu_r',
                center=0,
                vmin=-1,
                vmax=1,
                square=True,
                annot_kws={'size': 12},  # 히트맵 내 텍스트 크기
                cbar_kws={'label': 'Correlation Coefficient'})

    # 커버리지 정보를 텍스트로 추가
    #for i, col1 in enumerate(columns_to_analyze):
    #    for j, col2 in enumerate(columns_to_analyze):
    #        if i > j:  # 대각선 아래 부분만
    #            coverage = coverage_matrix.loc[col1, col2]
    #            plt.text(j, i, f'\n\n{coverage:.0f}%',
    #                    ha='center', va='center')

    #plt.title('Correlation Heatmap\n(Numbers below correlation values show data coverage)')
    #plt.tight_layout()

    return corr_matrix, coverage_matrix

def print_strong_correlations(corr_matrix, threshold=0.5):
    """
    강한 상관관계를 가진 열 쌍을 출력하는 함수

    Parameters:
    corr_matrix (pandas.DataFrame): 상관계수 행렬
    threshold (float): 출력할 최소 상관계수 절대값 (기본값 0.5)
    """
    print(f"\n=== 강한 상관관계 (|상관계수| >= {threshold}) ===")
    for col1 in corr_matrix.columns:
        for col2 in corr_matrix.columns:
            if col1 < col2:  # 중복 방지
                corr = corr_matrix.loc[col1, col2]
                if abs(corr) >= threshold:
                    print(f"{col1} - {col2}: {corr:.3f}")

# 히트맵 생성 및 상관계수 행렬 얻기
corr_matrix, coverage_matrix = create_correlation_heatmap(transformed_train_df)

# 강한 상관관계 출력 (절대값 0.6 이상)
print_strong_correlations(corr_matrix, threshold=0.6)

def analyze_nan_correlations(df, corr_matrix, min_correlation=0.5):
   # NaN이 있는 열 찾기
   nan_columns = df.columns[df.isna().any()].tolist()

   # 각 NaN이 있는 열에 대해 높은 상관관계를 가진 열 찾기
   for col in nan_columns:
       high_corr_cols = []
       for other_col in corr_matrix.columns:
           if col != other_col:
               corr = abs(corr_matrix.loc[col, other_col])
               if corr >= min_correlation:
                   high_corr_cols.append((other_col, corr))

       # NaN 개수와 높은 상관관계 열 출력
       nan_count = df[col].isna().sum()
       print(f"\n{col}: NaN 개수 = {nan_count}")
       if high_corr_cols:
           print("높은 상관관계를 가진 열:")
           for other_col, corr in sorted(high_corr_cols, key=lambda x: abs(x[1]), reverse=True):
               print(f"  - {other_col}: {corr:.3f}")
analyze_nan_correlations(transformed_train_df, corr_matrix, min_correlation=0.5)

"""## NaN 내용 채워넣기"""

from sklearn.linear_model import LinearRegression

def fill_nan_with_correlations(train_df, test_df=None, min_correlation=0.5, max_iterations=5):
   # 훈련 데이터 처리
   df_filled = train_df.copy()
   iteration_info = []
   models = {}  # 테스트 데이터에 적용할 모델 저장

   for iteration in range(max_iterations):
       print(f"\n=== 반복 {iteration + 1} ===")
       filled_in_iteration = []

       for col in df_filled.columns[df_filled.isna().any()]:
           nan_count = df_filled[col].isna().sum()
           print(f"\n처리 중인 열: {col}")
           print(f"NaN 개수: {nan_count}")

           # 상관관계 높은 열들 찾기
           corr_matrix = df_filled.corr()
           high_corr_cols = []
           for other_col in corr_matrix.columns:
               if col != other_col:
                   correlation = abs(corr_matrix.loc[col, other_col])
                   if correlation >= min_correlation:
                       high_corr_cols.append((other_col, correlation))
                       print(f"- 상관관계가 높은 열: {other_col} (상관계수: {correlation:.3f})")

           if not high_corr_cols:
               print("- 충분한 상관관계를 가진 열이 없음")
               continue

           high_corr_cols.sort(key=lambda x: abs(x[1]), reverse=True)

           filled = False
           for ref_col, corr in high_corr_cols:
               print(f"- 시도하는 참조열: {ref_col} (상관계수: {corr:.3f})")

               mask = df_filled[col].isna()
               valid_ref_mask = ~df_filled[ref_col].isna()

               if sum(mask & valid_ref_mask) == 0:
                   print("  - 채울 수 있는 NaN이 없음")
                   continue

               try:
                   train_mask = ~df_filled[col].isna() & ~df_filled[ref_col].isna()
                   if sum(train_mask) < 2:
                       print("  - 학습할 유효 데이터가 부족함")
                       continue

                   X = df_filled[ref_col].loc[train_mask].values.reshape(-1, 1)
                   y = df_filled[col].loc[train_mask].values

                   model = LinearRegression()
                   model.fit(X, y)

                   # 모델 저장
                   if col not in models:
                       models[col] = []
                   models[col].append((ref_col, model, corr))

                   pred_mask = mask & valid_ref_mask
                   X_pred = df_filled[ref_col].loc[pred_mask].values.reshape(-1, 1)
                   predicted_values = model.predict(X_pred)

                   df_filled.loc[pred_mask, col] = predicted_values

                   filled_count = sum(pred_mask)
                   filled_in_iteration.append({
                       'column': col,
                       'reference_column': ref_col,
                       'correlation': corr,
                       'filled_count': filled_count,
                       'remaining_nan': nan_count - filled_count
                   })

                   print(f"  - 성공적으로 {filled_count}개의 NaN 값을 채움")
                   filled = True
                   break

               except Exception as e:
                   print(f"  - 오류 발생: {str(e)}")

           if not filled:
               print("- 어떤 참조열로도 값을 채우지 못함")

       iteration_info.append(filled_in_iteration)

       if not filled_in_iteration:
           print("\n더 이상 채울 수 있는 값이 없음")
           break

   print("\n=== 전체 채우기 결과 ===")
   for i, iteration in enumerate(iteration_info):
       if iteration:
           print(f"\n반복 {i + 1}:")
           for fill_info in iteration:
               print(f"\n컬럼: {fill_info['column']}")
               print(f"- 참조 컬럼: {fill_info['reference_column']} (상관계수: {fill_info['correlation']:.3f})")
               print(f"- 채워진 개수: {fill_info['filled_count']}")
               print(f"- 남은 NaN 개수: {fill_info['remaining_nan']}")

   # 테스트 데이터가 있는 경우 처리
   if test_df is not None:
       test_filled = test_df.copy()
       test_fill_info = []

       for col, model_info in models.items():
           if col in test_df.columns and test_df[col].isna().any():
               for ref_col, model, corr in model_info:
                   if ref_col in test_df.columns:
                       mask = test_filled[col].isna() & ~test_filled[ref_col].isna()
                       if sum(mask) > 0:
                           X_pred = test_filled[ref_col].loc[mask].values.reshape(-1, 1)
                           predicted_values = model.predict(X_pred)
                           test_filled.loc[mask, col] = predicted_values

                           test_fill_info.append({
                               'column': col,
                               'reference_column': ref_col,
                               'correlation': corr,
                               'filled_count': sum(mask)
                           })

       return df_filled, test_filled, test_fill_info

   return df_filled

# 훈련 데이터만 처리
train_filled = fill_nan_with_correlations(transformed_train_df, min_correlation=0.5)

# 훈련과 테스트 데이터 모두 처리
train_filled, test_filled, test_fill_info = fill_nan_with_correlations(transformed_train_df, transformed_test_df, min_correlation=0.5)

# NaN이 있는 열과 개수 확인
nan_before = transformed_train_df.isna().sum()
nan_after = train_filled.isna().sum()

# 비교 결과 출력
print("=== NaN 채우기 결과 ===")
for col in transformed_train_df.columns:
   if nan_before[col] > 0:
       filled = nan_before[col] - nan_after[col]
       print(f"\n{col}:")
       print(f"- 원래 NaN 개수: {nan_before[col]}")
       print(f"- 채워진 개수: {filled}")
       print(f"- 남은 NaN 개수: {nan_after[col]}")

# NaN이 있는 열과 개수 확인
nan_before = transformed_test_df.isna().sum()
nan_after = test_filled.isna().sum()

# 비교 결과 출력
print("=== NaN 채우기 결과 ===")
for col in transformed_test_df.columns:
   if nan_before[col] > 0:
       filled = nan_before[col] - nan_after[col]
       print(f"\n{col}:")
       print(f"- 원래 NaN 개수: {nan_before[col]}")
       print(f"- 채워진 개수: {filled}")
       print(f"- 남은 NaN 개수: {nan_after[col]}")

train = train_filled.fillna(0.5)
test = test_filled.fillna(0.5)

X = train.drop('임신 성공 여부', axis=1)
y = train['임신 성공 여부']

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(random_state=42)

model.fit(X, y)

pred_proba = model.predict_proba(test)[:, 1]

sample_submission = pd.read_csv('/content/drive/MyDrive/lg aimers/sample_submission.csv')
sample_submission['probability'] = pred_proba

sample_submission.to_csv('./fill_nan_submit.csv', index=False)

from xgboost import XGBClassifier
model = XGBClassifier(
    n_estimators=100,      # 트리 개수
    learning_rate=0.1,     # 학습률
    max_depth=3,           # 트리 깊이
    random_state=42
)

model.fit(X, y)

!pip uninstall xgboost scikit-learn -y
!pip install xgboost==2.0.3 scikit-learn==1.4.0