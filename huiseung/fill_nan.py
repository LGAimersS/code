# -*- coding: utf-8 -*-
"""Fill_NaN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VEONiaX7Ro_ZUE7QBDccIInd8etXgMHu

<h2>Import</h2>
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

"""###<h2>Data Load</h2>"""

train = pd.read_csv('train.csv').drop(columns=['ID'])
test = pd.read_csv('/content/drive/MyDrive/lg aimers/test.csv').drop(columns=['ID'])/content/drive/MyDrive/lg aimers/

X = pd.concat([train.drop(columns=['임신 성공 여부']), test], axis=0).reset_index(drop=True)

"""###<h2>NaN값들이 있는 행들의 특징 분석하기</h2>"""

X_nan_rows = X[X.isna().any(axis=1)]
nan_info =X_nan_rows.isna().sum()
nan_info = nan_info[nan_info > 0]  # NaN이 있는 컬럼만 필터링
print(nan_info)

"""<h3>Case1:특정 시술 유형이 NaN인 데이터프레임 </h3>"""

Data_Case_1=X_nan_rows[X_nan_rows['특정 시술 유형'].isna()]
nan_info1 = Data_Case_1.isna().sum()
nan_info1 = nan_info1[nan_info1 > 0]  # NaN이 있는 컬럼만 필터링
print(nan_info1)

"""<h3>Case2:단일 배아 이식 여부가 NaN인 데이터프레임 </h3>"""

X_nan_rows = X_nan_rows.drop(Data_Case_1.index)
Data_Case_2 = X_nan_rows[X_nan_rows['단일 배아 이식 여부'].isna()]
nan_info2 = Data_Case_2.isna().sum()
nan_info2 = nan_info2[nan_info2 > 0]  # NaN이 있는 컬럼만 필터링
print(nan_info2)

"""<h4>Case2-1:임신 시도 또는 마지막 임신 경과 연수가 NaN인 데이터프레임</h4>"""

Data_Case_2_1 = Data_Case_2[Data_Case_2['임신 시도 또는 마지막 임신 경과 연수'].isna()]
nan_info2_1 = Data_Case_2_1.isna().sum()
nan_info2_1 = nan_info2_1[nan_info2_1 > 0]  # NaN이 있는 컬럼만 필터링
print(nan_info2_1)

"""<h4>Case2-2:임신 시도 또는 마지막 임신 경과 연수가 NaN이 아닌 데이터프레임</h4>"""

Data_Case_2_2 = Data_Case_2.drop(Data_Case_2_1.index)
nan_info2_2 = Data_Case_2_2.isna().sum()
nan_info2_2 = nan_info2_2[nan_info2_2 > 0]  # NaN이 있는 컬럼만 필터링
print(nan_info2_2)

"""<h3>Case3:배아 이식 경과일이 NaN인 데이터프레임 </h3>"""

X_nan_rows = X_nan_rows.drop(Data_Case_2.index)
Data_Case_3 = X_nan_rows[X_nan_rows['배아 이식 경과일'].isna()]
nan_info3 = Data_Case_3.isna().sum()
nan_info3 = nan_info3[nan_info3 > 0]  # NaN이 있는 컬럼만 필터링
print(nan_info3)

"""<h3>Case3-1:난자 채취 경과일이 NaN인 데이터프레임 </h3>"""

Data_Case_3_1 = Data_Case_3[Data_Case_3['난자 채취 경과일'].isna()]
nan_info3_1 = Data_Case_3_1.isna().sum()
nan_info3_1 = nan_info3_1[nan_info3_1 > 0]  # NaN이 있는 컬럼만 필터링
print(nan_info3_1)

"""<h4>Case3-2:난자 채취 경과일이 NaN이 아니고 난자 혼합 경과일이 NaN인 데이터프레임</h4>"""

Data_Case_3_2 = Data_Case_3.drop(Data_Case_3_1.index)
Data_Case_3_2 = Data_Case_3_1[Data_Case_3_1['배아 해동 경과일'].isna()]
nan_info3_2 = Data_Case_3_2.isna().sum()
nan_info3_2 = nan_info3_2[nan_info3_2 > 0]  # NaN이 있는 컬럼만 필터링
print(nan_info3_2)

"""<h4>Case3-2:임신 시도 또는 마지막 임신 경과 연수가 NaN이 아닌 데이터프레임</h4>"""

Data_Case_3_2 = Data_Case_3.drop(Data_Case_3_1.index)
nan_info3_2 = Data_Case_3_2.isna().sum()
nan_info3_2 = nan_info3_2[nan_info3_2 > 0]  # NaN이 있는 컬럼만 필터링
print(nan_info3_2)

"""<h3>Case4:난자 혼합 경과일이 NaN인 데이터프레임 </h3>"""

X_nan_rows = X_nan_rows.drop(Data_Case_3.index)
Data_Case_4 = X_nan_rows[X_nan_rows['난자 혼합 경과일'].isna()]
nan_info4 = Data_Case_4.isna().sum()
nan_info4 = nan_info4[nan_info4 > 0]  # NaN이 있는 컬럼만 필터링
print(nan_info4)

"""<h3>Case5:난자 채취 경과일이 NaN인 데이터프레임 </h3>"""

X_nan_rows = X_nan_rows.drop(Data_Case_4.index)
Data_Case_5 = X_nan_rows[X_nan_rows['난자 채취 경과일'].isna()]
nan_info5 = Data_Case_5.isna().sum()
nan_info5 = nan_info5[nan_info5 > 0]  # NaN이 있는 컬럼만 필터링
print(nan_info5)

"""<h3>Case6:임신 시도 또는 마지막 임신 경과 연수가 NaN인 데이터프레임 </h3>"""

X_nan_rows = X_nan_rows.drop(Data_Case_5.index)
Data_Case_6 = X_nan_rows[X_nan_rows['임신 시도 또는 마지막 임신 경과 연수'].isna()]
nan_info6 = Data_Case_6.isna().sum()
nan_info6 = nan_info6[nan_info6 > 0]  # NaN이 있는 컬럼만 필터링
print(nan_info6)

"""<h3>Case7:착상 전 유전 검사 사용 여부가 NaN인 데이터프레임 </h3>"""

X_nan_rows = X_nan_rows.drop(Data_Case_6.index)
Data_Case_7 = X_nan_rows[X_nan_rows['착상 전 유전 검사 사용 여부'].isna()]
nan_info7 = Data_Case_7.isna().sum()
nan_info7 = nan_info7[nan_info7 > 0]  # NaN이 있는 컬럼만 필터링
print(nan_info7)

"""<h3>Case8: PGD 시술 여부가 NaN인 데이터프레임 </h3>"""

X_nan_rows = X_nan_rows.drop(Data_Case_7.index)
Data_Case_8 = X_nan_rows[X_nan_rows['PGD 시술 여부'].isna()]
nan_info8 = Data_Case_8.isna().sum()
nan_info8 = nan_info8[nan_info8 > 0]  # NaN이 있는 컬럼만 필터링
print(nan_info8)

"""<h2>Case 정리</h2>"""

import pandas as pd
import numpy as np
from collections import defaultdict
import os
import hashlib
import json

def create_pattern_hash(pattern_columns):
    """
    컬럼 패턴을 해시값으로 변환하는 함수

    Parameters:
    pattern_columns (tuple): NaN인 컬럼들의 튜플

    Returns:
    str: 해시값의 첫 8자리
    """
    if not pattern_columns:
        return 'no_nan'

    # 컬럼명들을 정렬하고 결합하여 해시 생성
    pattern_str = '_'.join(sorted(pattern_columns))
    hash_object = hashlib.md5(pattern_str.encode())
    return hash_object.hexdigest()[:8]

def split_by_nan_pattern(train_df, test_df, base_dir='dataset_splits'):
    """
    Train과 Test 데이터셋을 NaN 패턴별로 분할하여 저장하는 함수
    긴 파일명을 피하기 위해 해시값을 사용하고 매핑 정보를 별도로 저장

    Parameters:
    train_df (pandas.DataFrame): 학습 데이터셋
    test_df (pandas.DataFrame): 테스트 데이터셋
    base_dir (str): 결과를 저장할 기본 디렉토리 경로

    Returns:
    dict: 각 패턴별로 train과 test 데이터프레임을 포함하는 딕셔너리
    """
    # 기본 디렉토리 생성
    os.makedirs(base_dir, exist_ok=True)

    # 패턴-해시 매핑 정보를 저장할 딕셔너리
    pattern_mapping = {}

    # 결과를 저장할 딕셔너리
    result = {}

    # train과 test에서 발견되는 모든 NaN 패턴을 수집
    all_patterns = set()

    for df in [train_df, test_df]:
        for _, row in df.iterrows():
            nan_columns = tuple(sorted(row[row.isna()].index.tolist()))
            all_patterns.add(nan_columns)

    for pattern in all_patterns:
        # 패턴의 해시값 생성
        pattern_hash = create_pattern_hash(pattern)

        # 매핑 정보 저장
        pattern_mapping[pattern_hash] = {
            'columns': list(pattern) if pattern else [],
            'pattern_name': 'No NaN' if not pattern else ', '.join(pattern)
        }

        # 패턴별 디렉토리 생성
        pattern_dir = os.path.join(base_dir, pattern_hash)
        os.makedirs(pattern_dir, exist_ok=True)

        # Train 데이터 처리
        train_mask = train_df.isna().apply(
            lambda row: set(row[row].index) == set(pattern),
            axis=1
        )
        train_subset = train_df[train_mask].copy()

        # Test 데이터 처리
        test_mask = test_df.isna().apply(
            lambda row: set(row[row].index) == set(pattern),
            axis=1
        )
        test_subset = test_df[test_mask].copy()

        # 파일로 저장
        train_path = os.path.join(pattern_dir, 'train.csv')
        test_path = os.path.join(pattern_dir, 'test.csv')

        train_subset.to_csv(train_path, index=True)
        test_subset.to_csv(test_path, index=True)

        # 결과 저장
        result[pattern_hash] = {
            'train': train_subset,
            'test': test_subset,
            'train_path': train_path,
            'test_path': test_path,
            'train_samples': len(train_subset),
            'test_samples': len(test_subset),
            'pattern_info': pattern_mapping[pattern_hash]
        }

        # 저장 결과 출력
        print(f"\nPattern Hash: {pattern_hash}")
        print(f"Pattern: {pattern_mapping[pattern_hash]['pattern_name']}")
        print(f"Train samples: {len(train_subset)}")
        print(f"Test samples: {len(test_subset)}")
        print(f"Saved to: {pattern_dir}")

    # 패턴 매핑 정보를 JSON 파일로 저장
    mapping_file = os.path.join(base_dir, 'pattern_mapping.json')
    with open(mapping_file, 'w', encoding='utf-8') as f:
        json.dump(pattern_mapping, f, ensure_ascii=False, indent=2)

    return result

# 매핑 정보를 로드하는 함수
def load_pattern_mapping(base_dir='dataset_splits'):
    """
    저장된 패턴 매핑 정보를 로드하는 함수

    Parameters:
    base_dir (str): 기본 디렉토리 경로

    Returns:
    dict: 패턴 매핑 정보
    """
    mapping_file = os.path.join(base_dir, 'pattern_mapping.json')
    with open(mapping_file, 'r', encoding='utf-8') as f:
        return json.load(f)

# train과 test 데이터프레임 로드
train_df = pd.read_csv('/content/drive/MyDrive/lg aimers/train.csv')
test_df = pd.read_csv('/content/drive/MyDrive/lg aimers/test.csv')

# 패턴별로 분할하고 저장
result = split_by_nan_pattern(train_df, test_df, base_dir='/content/drive/MyDrive/lg aimers/dataset_splits')

mapping = load_pattern_mapping()
for pattern_hash, info in mapping.items():
    print(f"\nHash: {pattern_hash}")
    print(f"Pattern: {info['pattern_name']}")