# -*- coding: utf-8 -*-
"""AI_minimum.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JUd08O02zdIZjkq-pSxyZweQUX5CSp53
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import lightgbm as lgb
import xgboost as xgb
from scipy.stats import mode

from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_selection import VarianceThreshold
from sklearn.utils.class_weight import compute_sample_weight
from torch.utils.data import WeightedRandomSampler

import os
os.environ["LOKY_MAX_CPU_COUNT"] = "4"

# Custom Dataset for PyTorch
class FertilityDataset(Dataset):
    def __init__(self, features, targets=None):
        if isinstance(features, (pd.DataFrame, pd.Series)):
            features = features.values
        self.features = torch.FloatTensor(features)

        if targets is not None:
            if isinstance(targets, pd.Series):
                targets = targets.values
            self.targets = torch.FloatTensor(targets)
        else:
            self.targets = None

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        if self.targets is not None:
            return self.features[idx], self.targets[idx]
        return self.features[idx]

# Training function for deep learning models
def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=20, scheduler=None):
    for epoch in range(epochs):
        # Training Phase
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for features, targets in train_loader:
            features, targets = features.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, targets.view(-1, 1))
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = (outputs.data > 0.5).float()
            train_total += targets.size(0)
            train_correct += (predicted.view(-1) == targets).sum().item()

        avg_train_loss = train_loss / len(train_loader)
        train_accuracy = 100 * train_correct / train_total

        # Validation Phase
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for features, targets in val_loader:
                features, targets = features.to(device), targets.to(device)
                outputs = model(features)
                loss = criterion(outputs, targets.view(-1, 1))

                val_loss += loss.item()
                predicted = (outputs.data > 0.5).float()
                val_total += targets.size(0)
                val_correct += (predicted.view(-1) == targets).sum().item()

        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = 100 * val_correct / val_total

        if scheduler is not None:
            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                scheduler.step(avg_val_loss)
            else:
                scheduler.step()

        print(f'Epoch [{epoch+1}/{epochs}]')
        print(f'Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')
        print(f'Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')
        print('-' * 60)

    return model

# Neural Network Models
# Improved Transformer Model
class EnhancedTransformerModel(nn.Module):
    def __init__(self, input_dim, num_heads=8, num_layers=3, dropout=0.2):
        super().__init__()
        self.embedding1 = nn.Linear(input_dim, 256)
        self.embedding2 = nn.Linear(input_dim, 128)

        encoder_layer1 = nn.TransformerEncoderLayer(
            d_model=256,
            nhead=num_heads,
            dropout=dropout,
            activation='gelu'
        )
        encoder_layer2 = nn.TransformerEncoderLayer(
            d_model=128,
            nhead=num_heads//2,
            dropout=dropout,
            activation='gelu'
        )

        self.transformer1 = nn.TransformerEncoder(encoder_layer1, num_layers)
        self.transformer2 = nn.TransformerEncoder(encoder_layer2, num_layers)

        self.fc = nn.Sequential(
            nn.Linear(384, 192),
            nn.SELU(),
            nn.Dropout(dropout),
            nn.Linear(192, 96),
            nn.SELU(),
            nn.Dropout(dropout),
            nn.Linear(96, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x1 = self.embedding1(x).unsqueeze(1)
        x2 = self.embedding2(x).unsqueeze(1)

        x1 = self.transformer1(x1).squeeze(1)
        x2 = self.transformer2(x2).squeeze(1)

        x_combined = torch.cat([x1, x2], dim=1)
        return self.fc(x_combined)

# Improved CNN Model
class EnhancedCNNModel(nn.Module):
    def __init__(self, input_dim):
        super().__init__()

        def conv_block(in_channels, out_channels):
            return nn.Sequential(
                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),
                nn.BatchNorm1d(out_channels),
                nn.SELU(),
                nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),
                nn.BatchNorm1d(out_channels),
                nn.SELU(),
            )

        self.proj1 = nn.Conv1d(32, 64, kernel_size=1)
        self.proj2 = nn.Conv1d(64, 128, kernel_size=1)
        self.input_proj = nn.Conv1d(1, 32, kernel_size=1)
        self.conv1 = conv_block(32, 64)
        self.conv2 = conv_block(64, 128)
        self.conv3 = conv_block(128, 256)
        self.pool = nn.AdaptiveAvgPool1d(16)
        self.fc = nn.Sequential(
            nn.Linear(256 * 16, 512),
            nn.SELU(),
            nn.Dropout(0.3),
            nn.Linear(512, 128),
            nn.SELU(),
            nn.Dropout(0.2),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.input_proj(x)

        identity = x
        x = self.conv1(x)
        identity = self.proj1(identity)
        x = x + identity

        identity = x
        x = self.conv2(x)
        identity = self.proj2(identity)
        x = x + identity

        x = self.conv3(x)
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        return self.fc(x)

class EnhancedFertilityEnsemble:
    def __init__(self, n_components=150):
        self.n_components = n_components
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=n_components)
        self.transformer = None
        self.cnn = None
        self.lgbm = None
        self.xgb = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    def prepare_data(self, X):
        if 'ID' in X.columns:
            X = X.drop('ID', axis=1)
        elif 'id' in X.columns:
            X = X.drop('id', axis=1)

        if isinstance(X, pd.DataFrame):
            poly = PolynomialFeatures(degree=2, include_bias=False)
            numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns
            X_poly = poly.fit_transform(X[numeric_cols])
            X_poly_df = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(numeric_cols))

            selector = VarianceThreshold(threshold=0.01)
            X_poly_selected = selector.fit_transform(X_poly_df)
            X_poly_cols = X_poly_df.columns[selector.get_support()]

            X = pd.concat([X, pd.DataFrame(X_poly_selected, columns=X_poly_cols)], axis=1)
            X = X.values

        X_scaled = self.scaler.fit_transform(X)
        X_pca = self.pca.fit_transform(X_scaled)
        return X_pca, X_scaled

    def train(self, X, y, batch_size=32):
        X_pca, X_scaled = self.prepare_data(X)

        # Train/Validation split (80-20)
        train_size = int(0.8 * len(X))

        # Indices for splitting
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        train_idx, val_idx = indices[:train_size], indices[train_size:]

        # Split data
        X_train_pca, X_val_pca = X_pca[train_idx], X_pca[val_idx]
        X_train_scaled, X_val_scaled = X_scaled[train_idx], X_scaled[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        # Prepare data loaders
        train_dataset = FertilityDataset(X_train_pca, y_train)
        val_dataset = FertilityDataset(X_val_pca, y_val)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size)

        # Train Transformer
        self.transformer = EnhancedTransformerModel(self.n_components).to(self.device)
        optimizer = torch.optim.AdamW(self.transformer.parameters(), lr=1e-4, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
        criterion = nn.BCELoss()
        self.transformer = train_model(
            self.transformer, train_loader, val_loader, criterion, optimizer, self.device,
            epochs=20, scheduler=scheduler
        )

        # Train CNN
        self.cnn = EnhancedCNNModel(self.n_components).to(self.device)
        optimizer = torch.optim.AdamW(self.cnn.parameters(), lr=1e-4, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
        self.cnn = train_model(
            self.cnn, train_loader, val_loader, criterion, optimizer, self.device,
            epochs=20, scheduler=scheduler
        )

        # Train LightGBM
        self.lgbm = lgb.LGBMClassifier(
            n_estimators=1000,
            learning_rate=0.01,
            num_leaves=31,
            max_depth=7,
            min_child_samples=20,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42
        )
        self.lgbm.fit(X_train_scaled, y_train)

        # Train XGBoost
        self.xgb = xgb.XGBClassifier(
            n_estimators=1000,
            learning_rate=0.01,
            max_depth=6,
            min_child_weight=1,
            gamma=0,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss'
        )
        self.xgb.fit(X_train_scaled, y_train)

        # Make predictions for validation set
        self.transformer.eval()
        self.cnn.eval()
        with torch.no_grad():
            val_tensor = torch.FloatTensor(X_val_pca).to(self.device)
            transformer_preds = self.transformer(val_tensor).cpu().numpy().ravel()
            cnn_preds = self.cnn(val_tensor).cpu().numpy().ravel()

        lgbm_preds = self.lgbm.predict_proba(X_val_scaled)[:, 1]
        xgb_preds = self.xgb.predict_proba(X_val_scaled)[:, 1]

        # Weighted ensemble predictions
        ensemble_preds = (
            0.3 * transformer_preds +
            0.3 * cnn_preds +
            0.2 * lgbm_preds +
            0.2 * xgb_preds
        )
        final_preds = (ensemble_preds > 0.5).astype(int)

        # Calculate metrics
        accuracy = accuracy_score(y_val, final_preds)
        auc = roc_auc_score(y_val, ensemble_preds)

        return {
            'accuracy': accuracy,
            'auc': auc,
            'predictions': final_preds,
            'probabilities': ensemble_preds
        }

    def predict(self, X):
        X_pca, X_scaled = self.prepare_data(X)
        X_tensor = torch.FloatTensor(X_pca).to(self.device)

        self.transformer.eval()
        self.cnn.eval()
        with torch.no_grad():
            transformer_preds = self.transformer(X_tensor).cpu().numpy().ravel()
            cnn_preds = self.cnn(X_tensor).cpu().numpy().ravel()

        lgbm_preds = self.lgbm.predict_proba(X_scaled)[:, 1]
        xgb_preds = self.xgb.predict_proba(X_scaled)[:, 1]

        # Use dynamic weights for ensemble predictions
        ensemble_preds = (
            self.weights['transformer'] * transformer_preds +
            self.weights['cnn'] * cnn_preds +
            self.weights['lgbm'] * lgbm_preds +
            self.weights['xgb'] * xgb_preds
        )
        return (ensemble_preds > 0.5).astype(int), ensemble_preds

# Main execution
if __name__ == "__main__":
    # Load data
    train_data = pd.read_csv('/content/drive/MyDrive/lg aimers/transformed_train_df.csv')
    test_data = pd.read_csv('/content/drive/MyDrive/lg aimers/transformed_test_df.csv')
    sample_submission = pd.read_csv('/content/drive/MyDrive/lg aimers/sample_submission.csv')

    X = train_data.drop('임신 성공 여부', axis=1)
    y = train_data['임신 성공 여부']

    # Train model
    ensemble = EnhancedFertilityEnsemble(n_components=150)
    results = ensemble.train(X, y, batch_size=32)

    # Make predictions on test data
    predictions, probabilities = ensemble.predict(test_data)

    # Save predictions
    sample_submission['probability'] = probabilities
    sample_submission.to_csv('ensemble_submission.csv', index=False)

    # Print results
    print("\nTraining Results:")
    print(f"Accuracy: {results['accuracy']:.4f}")
    print(f"AUC: {results['auc']:.4f}")
    print("\nPrediction Distribution:")
    print(sample_submission['probability'].describe())